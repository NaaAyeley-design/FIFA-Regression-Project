# -*- coding: utf-8 -*-
"""Regression Problem: FIFA

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IhZKxsItqu1jp9mh1qTM6VYPW1g09zPv
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn .ensemble import GradientBoostingRegressor
from sklearn.ensemble import VotingRegressor
from sklearn.model_selection import cross_val_score, KFold,GridSearchCV
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
from google.colab import drive
drive.mount('/content/drive')

players_22_path = '/content/drive/MyDrive/players_22.csv'
legacy_21_path = '/content/drive/MyDrive/male_players (legacy).csv'


players_22 = pd.read_csv(players_22_path)
male_legacy21 = pd.read_csv(legacy_21_path)

male_legacy21.hist(bins=50, figsize = (20,15))
plt.show()

male_legacy21df = pd.DataFrame(male_legacy21)
limit = 0.3*len(male_legacy21df)

# Drop columns with more than 03% missing values
male_legacy21df = male_legacy21df.dropna(thresh=limit, axis=1)

# Define irrelevant columns
print(male_legacy21df.columns)
drop_columns = [
    'player_url', 'player_positions', 'nationality_id', 'player_face_url', 'fifa_update_date',
    'club_jersey_number', 'club_joined_date', 'club_contract_valid_until_year', 'long_name', 'dob',
    'short_name', 'club_team_id', 'player_url', 'real_face', 'club_team_id', 'club_name',
    'league_name', 'nationality_id', 'nationality_name', 'club_position', 'ls', 'st', 'rs', 'lw',
    'lf', 'cf', 'fifa_version', 'rf', 'rw', 'lam', 'cam', 'ram', 'lm', 'lcm', 'cm', 'rcm', 'rm',
    'lwb', 'ldm', 'cdm', 'rdm', 'rwb', 'lb', 'lcb', 'cb', 'player_traits', 'rcb', 'rb', 'gk',
    'league_id'
]

# Drop irrelevant columns
existing_drop_columns = [col for col in drop_columns if col in male_legacy21df.columns]
male_legacy21df = male_legacy21df.drop(existing_drop_columns, axis=1)

# Split numeric and non-numeric data
numeric_data = male_legacy21df.select_dtypes(include=np.number)
non_numeric = male_legacy21df.select_dtypes(include=['object'])

# Fill missing values with the mean for numeric data
numeric_data = numeric_data.fillna(numeric_data.mean())

# Convert non-numeric data to numeric using one-hot encoding
non_numeric = pd.get_dummies(non_numeric).astype(int)
# Perform multivariate imputation on numeric data
imp = IterativeImputer(max_iter=10, random_state=0)
numeric_data = pd.DataFrame(np.round(imp.fit_transform(numeric_data)), columns=numeric_data.columns)

# Combine the numeric and non-numeric data
combine_dataframe = pd.concat([numeric_data, non_numeric], axis=1)

# Ensure no missing values remain
combine_dataframe = combine_dataframe.fillna(combine_dataframe.mean())

# Select the top 11 relevant columns based on correlation with the target variable 'overall'
corr_matrix = combine_dataframe.corr()
using_columns = corr_matrix["overall"].sort_values(ascending=False).index[:9]
combine_dataframe = combine_dataframe[using_columns]

# Split features and target variable
y = combine_dataframe["overall"]
x = combine_dataframe.drop("overall", axis=1)

# Scale the features
scale = StandardScaler()
x = scale.fit_transform(x)

# Split the data into training and testing sets
xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2, random_state=42)

corr_matrix = combine_dataframe.corr()
using_columns = corr_matrix["overall"].sort_values(ascending=False).index[:9]
combine_dataframe = combine_dataframe[using_columns]
combine_dataframe = combine_dataframe.sample(n=20000, random_state = 42)
combine_dataframe = combine_dataframe[:20000]

fold_number = 10
kf  = KFold(n_splits = fold_number, shuffle = True, random_state = 42)

from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor

from sklearn.model_selection import GridSearchCV, cross_val_score, KFold

from sklearn.metrics import mean_squared_error

import numpy as np

"""Training for gradient Boosting"""

gradient_boosting = GradientBoostingRegressor(n_estimators=500, random_state=42, max_depth=4, min_samples_split = 2, learning_rate= 0.01)

GBscore = cross_val_score(gradient_boosting, xtrain, ytrain, cv = kf, scoring = 'neg_mean_squared_error')

RMSE_for_gradientboosting = np.sqrt(-GBscore.mean())

gradient_boosting.fit(xtrain, ytrain)

score =  gradient_boosting.score(xtest, ytest)

RMSE = np.sqrt(mean_squared_error(ytest, gradient_boosting.predict(xtest)))
print("RMSE: %.4f" % RMSE)
print("size of prediction: ", len(gradient_boosting.predict(xtest)))
print("prediction: \n", gradient_boosting.predict(xtest))
print("test score: {0:.4f}\n".format(score))

# Training the XGBoost Model
import xgboost as xgb

"""

> Training the XGBoost Model

"""

xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)
xgbscore = cross_val_score(xgb_model, xtrain, ytrain, cv = kf,scoring = 'neg_mean_squared_error')
RMSE_for_xgb_R = np.sqrt(-xgbscore.mean())
xgb_model.fit(xtrain,ytrain)
score =  xgb_model.score(xtest, ytest)
RMSE = np.sqrt(mean_squared_error(ytest, xgb_model.predict(xtest)))



print(f"""Mean Squared Error = {mean_squared_error(ytest, xgb_model.predict(xtest))},
Root Mean Squared Error = {RMSE}""")

"""Training a RandomForest Model

"""

# Training the RandomForest Model
RF_R = RandomForestRegressor(n_estimators = 100,random_state = 42)
RFscore = cross_val_score(RF_R,xtrain,ytrain,cv = kf,scoring = 'neg_mean_squared_error')
RMSE_for_RF_R = np.sqrt(-RFscore.mean())
RF_R.fit(xtrain,ytrain)
score = RF_R.score(xtest,ytest)
rmse = np.sqrt(mean_squared_error(ytest,RF_R.predict(xtest)))

print("RMSE: %.4f" % rmse)
print("size of prediction: ", len(RF_R.predict(xtest)))
print("prediction: \n", RF_R.predict(xtest))
print("test score: {0:.4f}\n".format(score))

"""Optimization for XGBoost Model"""

params = {'n_estimators': 10000, 'max_depth': 5, 'subsample': 0.8,
          'learning_rate': 0.01}

optimized_xgb = xgb.XGBRegressor(**params)
optimized_xgb.fit(xtrain,ytrain)

score = optimized_xgb.score(xtest,ytest)
y_pred = optimized_xgb.predict(xtest)
mse = mean_squared_error(ytest,y_pred)
rmse = np.sqrt(mse)


print("RMSE: %.4f" % rmse)
print("size of prediction: ", len(y_pred))
print("prediction: \n", y_pred)
print("test score: {0:.4f}\n".format(score))

"""Optimization for Gradient Boosting"""

params = {'n_estimators': 10000, 'max_depth': 5, 'min_samples_split': 2,
          'learning_rate': 0.01, 'loss': 'squared_error'}
optimized_GB = GradientBoostingRegressor(**params)
optimized_GB.fit(xtrain,ytrain)
score = optimized_GB.score(xtest,ytest)
y_pred = optimized_GB.predict(xtest)
mse = mean_squared_error(ytest,y_pred)
rmse = np.sqrt(mse)

print("RMSE: %.4f" % rmse)
print("size of prediction: ", len(y_pred))
print("prediction: \n", y_pred)
print("test score: {0:.4f}\n".format(score))

"""Optimization of the RandomForest Model"""

params = {
    'n_estimators': 100,
    'max_depth': 5,
    'min_samples_split': 2,
    'min_samples_leaf': 1,
    'max_features': 'auto',
    'bootstrap': True,
    'random_state': 42
}

# Instantiate the model with the defined hyperparameters
optimized_rf = RandomForestRegressor(**params)

# Train the model
optimized_rf.fit(xtrain, ytrain)

# Predict and evaluate
y_pred = optimized_rf.predict(xtest)
score = optimized_rf.score(xtest, ytest)
mse = mean_squared_error(ytest, y_pred)
rmse = np.sqrt(mse)
print("RMSE: %.4f" % rmse)
print("size of prediction: ", len(y_pred))
print("prediction: \n", y_pred)
print("test score: {0:.4f}\n".format(score))

ensemble = VotingRegressor(estimators=[
    ('optimized_GradientBoostingRegressor', optimized_GB),
    ('optimized_xgb', optimized_xgb),
    ('optimized_random forest regressor', optimized_rf),
])

ensemble.fit(xtrain, ytrain)
# Make predictions using the ensemble modelThis VotingRegressor instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.This VotingRegressor instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.
y_pred = ensemble.predict(xtest)
score = ensemble.score(xtest, ytest)


# Calculate RMSE (Root Mean Squared Error)
mse = mean_squared_error(ytest, y_pred)

print("MSE: %.4f" % mse)
print("size of prediction: ", len(ensemble.predict(xtest)))
print("prediction: \n", ensemble.predict(xtest))
print("test score: {0:.4f}\n".format(score))

def preprocessing(data,feature_names):
  # Drop columns with more than 30% missing values
  threshold = 0.3*len(data)
  data = data.dropna(thresh=threshold, axis=1)

  relevant_columns = ['overall','movement_reactions','potential','passing','wage_eur','mentality_composure','value_eur','dribbling','attacking_short_passing','mentality_vision','international_reputation']
  data = data[relevant_columns]

  imp = IterativeImputer(max_iter = 10, random_state = 0)
  data = pd.DataFrame(np.round(imp.fit_transform(data)),columns = data.columns)
  #combine both numeric and non-numeric dataframes
  X = data
  #fill missing values with mean
  X = X.fillna(X.mean())


# Subset data
  X = X[relevant_columns]
  y = X["overall"]
  x = X.drop("overall", axis=1)
  X = X[feature_names]

  # Scale the features
  scale = StandardScaler()
  scaled = scale.fit_transform(x)
  x = pd.DataFrame(scaled, columns=x.columns)

  X = pd.concat([x, y.reset_index(drop=True)], axis=1)

  return X
  print(X.head())

"""Testing with the 22_players data"""

players_22df = pd.DataFrame(players_22)
feature_names = ['passing','wage_eur','mentality_composure','value_eur','dribbling','attacking_short_passing','mentality_vision','international_reputation']
data_22 = preprocessing(players_22df, feature_names)

X_22 = data_22.drop(columns=['overall'])
y_22 = data_22['overall']

X_22.head()

X_22 = X_22[feature_names]
score = ensemble.score(X_22,y_22)
y_pred = ensemble.predict(X_22)
mse = mean_squared_error(y_22,y_pred)
rmse = np.sqrt(mse)

print("RMSE: %.4f" % np.sqrt(mse))
print("size of prediction: ", len(ensemble.predict(X_22)))
print("prediction: \n", ensemble.predict(X_22))
print("test score: {0:.4f}\n".format(score))

score = optimized_xgb.score(X_22, y_22)


# Make predictions using the ensemble model
y_pred_22 = optimized_GB.predict(X_22)

# Calculate RMSE (Root Mean Squared Error)
mse = mean_squared_error(y_22, y_pred_22)

print("MSE: %.4f" % mse)
print("size of prediction: ", len(optimized_GB.predict(X_22)))
print("prediction: \n", optimized_GB.predict(X_22))
print("test score: {0:.4f}\n".format(score))

import joblib
joblib.dump(ensemble, '/content/drive/My Drive/model.pkl', compress=8)